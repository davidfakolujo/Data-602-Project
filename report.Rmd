---
title: "**Statistical Inference in a Cocaine Drug Bust**"
subtitle: "DATA 602, Winter 2025"
author: "David Fakolujo, Akinyemi Apampa, Ravin Jayasuriya, Prince Oloma and Joshua Ogunbo"
output: 
  pdf_document:
    latex_engine: xelatex
fontsize: 14pt
geometry: margin=1in
---

## **Background**

A cocaine bust in Calgary yielded **N₁ = 496** suspected cocaine plastic packets. To convict the suspected drug traffickers, the **Alberta Crown Prosecution Service (ACPS)** and the **Calgary Police Service (CPS)** had to prove that there was **genuine cocaine** in (at least one of) the packets.  

Apparently, drug traffickers have been **mixing “clean” packets** (i.e., packets that are negative for cocaine, often containing corn starch) with **“dirty” packets** (i.e., packets that are positive for cocaine) to confound the police.

Due to **budget limitations** or a **lack of resources**, law enforcement is often restricted to testing **a smaller sample** of the total shipment. This raises the question:  

> *How can statistical inference be used to estimate the number of contaminated packets with minimal testing?*

By analyzing this problem using **statistical methods**, we aim to demonstrate how **statistical inference can be utilized in law enforcement** to assist in decision-making.


The goal of this project is to develop a **statistical inference method** to determine the **total number of contaminated packets** within a cocaine shipment. This estimation is crucial for **legal proceedings**, as it determines whether there is **sufficient evidence** for conviction.


## **Methods**

To estimate **$\theta$**, the total number of contaminated packets, we will use the following statistical methods:

(1) **Hypergeometric Distribution:** Models the probability of selecting **dirty packets** in a **limited sample** without replacement.

$$
P_{\theta}(X_1 = x_1) = \frac{\binom{\theta}{x_1} \binom{N_1 - \theta}{n_1 - x_1}}{\binom{N_1}{n_1}}
$$
 \ 
 \
 \
 \
where:

-   $N_1$ - The total number of packets,
-   $n_1$ - The sample size,
-   $\theta$ is the unknown number of dirty packets,
-   $x_1$ represents the observed number of dirty packets,
-   $N_1 - \theta$ is the number of clean packets in the population,
-   $n_1 - x_1$ is the number of clean packets from the sample.
\

(2) **Maximum Likelihood Estimation (MLE):** This was used to determine the total number of dirty packets,**$\theta$**, in the parameter space containing all the possible values of **$\theta$**, which has the highest probability in the hypergeometric distribution and negative hypergeometric distribution

$$
\hat{\theta}_1 = \underset{\theta \in \Theta(x_1)}{\arg\max} P_{\theta}(X_1 = x_1).
$$

\
\
\

(3) **Monte Carlo Simulation**: Monte Carlo simulation was used to evaluate and compare the performance of the two estimation approaches by repeatedly generating synthetic data and applying the estimation methods. The process involved the following steps:

- **Hypergeometric Distribution:**  
  - Generated **$N$** random samples of dirty packets based on the observed sample size.  
  - Applied the **hypergeometric function** to each sample to estimate the **maximum likelihood estimate (MLE)** of the total number of contaminated packets ($\theta$) across various sample sizes.  

- **Negative Hypergeometric Distribution:**  
  - Similarly, generated **$N$** random samples of dirty packets under a **negative hypergeometric** framework.  
  - Applied the **negative hypergeometric function** to estimate $\theta$ for each generated sample using MLE.  

These simulations provided insights into the **bias, variance, and accuracy** of the estimation methods, ensuring that the chosen approach performs well under repeated sampling conditions.


## **Analysis and Results **

### Scenario 1

Our analysis involves taking a fixed sample of size \( n = 2 \) from a population of \( N = 496 \). We then evaluate different cases where the number of dirty packets in the sample, \( x_1 \), can take values from \( \{0,1,2\} \).

Using Maximum Likelihood Estimation (MLE), we compute the likelihood function for all values of \( \theta \), and identify the value of \( \theta \) that maximizes the likelihood.


### **Case 1: Estimating** $\theta$ When $X_1 = 1$

```{r, fig.width=4, fig.height=3, echo=FALSE}
N_1 <- 496  # Total packets
n_1 <- 2    # Sample size
x_1 <- 1    # Observed dirty packets

possible_theta <- 1:495

probability <- dhyper(x_1, possible_theta, N_1 - possible_theta, n_1)

theta_max <- possible_theta[which.max(probability)]
max_prob <- max(probability)

plot(
  possible_theta,
  probability,
  type = "b",
  col = "red",
  lwd = 0.5,
  xlab = expression(theta),
  ylab = "Probability",
  main = expression(paste("MLE of ", theta, " for ", X[1], "=1"))
)

points(theta_max, max_prob, col = "yellow", pch = 19, cex = 1.5)

print(paste("The maximum likelihood estimate for theta is:", theta_max))
print(paste("The maximum probability is:", max_prob))
```

From the output, the **Maximum Likelihood Estimate (MLE)** indicates
that the most likely number of dirty packets in the population is
$\theta = 248$.

This means that, given a sample of **2 packets**, where **1 was found to
be dirty**, the best estimate for the total number of dirty packets in
the entire population is **248**.

Additionally, the **likelihood** of observing exactly **1 dirty packet**
in the sample, assuming that $\theta = 248$, is approximately **50.1%**.

### Case 2: Estimating $\theta$ When $X_1 = 0$

```{r, fig.width=4, fig.height=3, echo=FALSE}
N_1 <- 496  # Total packets
n_1 <- 2    # Sample size
x_1 <- 0    # Observed dirty packets

possible_theta <- 0:494

probability <- dhyper(x_1, possible_theta, N_1 - possible_theta, n_1)

theta_max <- possible_theta[which.max(probability)]
max_prob <- max(probability)

# Plot likelihood function
plot(
  possible_theta,
  probability,
  type = "b",
  col = "red",
  lwd = 0.5,
  xlab = expression(theta),
  ylab = "Probability",
  main = expression(paste("MLE of ", theta, " for ", X[1], "=0"))
)

points(theta_max, max_prob, col = "yellow", pch = 19, cex = 1.5)

print(paste("The maximum likelihood estimate for theta is:", theta_max))
print(paste("The maximum probability is:", max_prob))
```

\

When no dirty packets ($x_1 = 0$) are observed in the sample, the
**Maximum Likelihood Estimate (MLE)** suggests that the most likely
value of $\theta$ (the total number of dirty packets in the population)
is **0**.

This conclusion makes intuitive sense if none of the selected packets
tested positive for cocaine, the best estimate is that there are **no
dirty packets** in the entire population. In other words, based on the
sample data, it is most probable that all packets in the population are
clean.

Mathematically, this means that the **likelihood function** reaches its
highest value when $\theta = 0$, reinforcing the idea that the absence
of dirty packets in the sample strongly suggests their absence in the
entire population.

### Case 3: Estimating $\theta$ When $X_1 = 2$

```{r, fig.width=4, fig.height=3, echo=FALSE}
N_1 <- 496  # Total packets
n_1 <- 2    # Sample size
x_1 <- 2    # Observed dirty packets

possible_theta_2 <- 2:496

probability_2 <- dhyper(x_1, possible_theta_2, N_1 - possible_theta_2, n_1)

theta_max_2 <- possible_theta_2[which.max(probability_2)]
max_prob_2 <- max(probability_2)

# Plot likelihood function
plot(
  possible_theta_2,
  probability_2,
  type = "b",
  col = "red",
  lwd = 0.5,
  xlab = expression(theta),
  ylab = "Probability",
  main = expression(paste("MLE of ", theta, " for ", X[1], "=2"))
)

points(theta_max_2, max_prob_2, col = "yellow", pch = 19, cex = 1.5)

print(paste("The maximum likelihood estimate for theta is:", theta_max_2))
print(paste("The maximum probability is:", max_prob_2))
```

\
When two dirty packets ($x_1 = 2$) are observed in the sample, the
**Maximum Likelihood Estimate (MLE)** suggests that the most likely
value of $\theta$ (the total number of dirty packets in the population)
is **496**.

This conclusion makes intuitive sense if two of the selected packets
tested positive for cocaine, the best estimate is that there are **496
dirty packets** in the entire population. In other words, based on the
sample data, it is most probable that all packets in the population are
dirty

Mathematically, this means that the **likelihood function** reaches its
highest value when $\theta = 496$, reinforcing the idea that the
presence of dirty packets in the sample strongly suggests their prsence
in the entire population.


\

## Scenario 2
We analyzed the joint sampling distribution of two dependent discrete random variables, \(X_1\) and \(X_2\), representing the number of dirty packets found in two successive rounds of random sampling without replacement. Initially, \(n_1 = 2\) packets were tested from a total of \(N_1 = 496\), and both were found clean. Subsequently, the investigators selected \(n_2 = 4\) additional packets from the remaining \(N_2 = 494\) packets, hoping to detect at least one dirty packet to support their case. 


After the initial test of $n_1 = 2$ packets resulted in $X_1 = 0$ dirty
packets, additional $n_2 = 4$ packets were tested to gather sufficient
evidence. Let $X_2$ represent the number of dirty packets in this second
sample.

### Joint Probability of $X_1$ and $X_2$

The joint probability function is given by:

$$
P_{\theta}(X_1 = x_1, X_2 = x_2) = P_{\theta}(X_2 = x_2 | X_1 = x_1) P_{\theta}(X_1 = x_1)
$$

Given that:

-   $X_1 \sim \text{Hypergeometric}(N_1 = 496, \theta, n_1 = 2)$,
-   $X_2 | X_1 = x_1 \sim \text{Hypergeometric}(N_2 = 496 - 2, \theta - x_1, n_2 = 4)$,

we define the possible values of $\theta$.

```{r, fig.width=4, fig.height=3, echo=FALSE}
# Observing 2 dirty packets out of 4
N_2 <- 494
n_2 <- 4
x_2 <- 2

# The possible theta values
possible_theta_3 <- 2:492

# Calculating the hypergeometric probability
probability_3 <- dhyper(x_2, possible_theta_3, N_2 - possible_theta_3, n_2)

# Finding the max values for theta and the max probability
theta_max_3 <- possible_theta_3[which.max(probability_3)]
max_prob_3 <- max(probability_3)


plot(
  possible_theta_3,
  probability_3,
  type = "b", 
  col = "red", 
  lwd = 0.5,
  xlab = expression(theta),
  ylab = "Probability",
  main = expression(paste("Probability Function for ", theta, " at ", X[2], "=2"))
)

points(theta_max_3, max_prob_3, col = "yellow", pch = 19, cex = 1.5)

max_theta = min(max(possible_theta_3), max(possible_theta_3))
min_theta = max(min(possible_theta_3), min(possible_theta_3))

print(paste("The Maximum Likelihood Estimate for theta is:", theta_max_3))
print(paste("The maximum Probability is:", max_prob_3))

print(
  paste(
    "The smallest possible value of theta for the joint sampling distribution is:", 
    min_theta
  )
)
print(
  paste(
    "The largest possible value of theta for the joint sampling distribution is:", 
    max_theta
  )
)
```

## Scenario 3
Given that \( x_2 = 2 \) out of the additional \( n_2 = 4 \) sampled packets were found to be dirty, the conviction of the accused is now certain. However, the total number of dirty packets, \( \theta \), remains unknown and needs to be estimated. $\newline$ 
 **Maximum Likelihood Estimation (MLE)**  
$\newline$
We determine \( \theta \), the **maximum likelihood estimate (MLE)** by maximizing the joint probability \( P_{\theta}(X_1 = x_1, X_2 = x_2) \) as a function of \( \theta \) over its feasible parameter space, \( \Theta(x_1, x_2) \), which depends on the observed values \( x_1 \) and \( x_2 \). Since both \( X_1(\theta) \) and \( X_2(\theta) \) are functions of \( \theta \), the possible values of \( \theta \) must be considered. 
## Finding the MLE 
We obtain The MLE, denoted as \( \hat{\theta}_2 \),  by evaluating \( P_{\theta}(X_1 = 0, X_2 = 2) \) for all possible values of \( \theta \) in \( \Theta(0,2) \) and selecting the value that maximizes this probability. This approach ensures that the estimated \( \theta \) is the most likely given the observed data.


With \( x_2 = 2 \) dirty packets found in the second test, the conviction is certain, but the actual number of dirty packets remains unknown. To estimate \( \theta \), we maximize the joint probability function:

\[
\hat{\theta}_2 = \underset{\theta \in \Theta(0,2)}{\arg\max} P_{\theta}(X_1 = 0, X_2 = 2)
\]

The new parameter space \( \Theta(0,2) \) depends on the observed values of \( X_1 \) and \( X_2 \).

```{r, fig.width=4, fig.height=3, echo=FALSE}
probability_4 = probability[3:493] * probability_3[1:491]

theta_max_4 <- possible_theta_2[which.max(probability_4)]
max_prob_4 <- max(probability_4)


plot(
  possible_theta_3,
  probability_4,
  type = "b", 
  col = "red", 
  lwd = 0.5,
  xlab = expression(theta),
  ylab = "Probability",
  main = expression(paste("Joint Probability Function for ", theta, " at ", 
                          X[1], "=0", " ", X[2], "=2"))
)

points(theta_max_4, max_prob_4, col = "yellow", pch = 19, cex = 1.5)

print(paste("The Maximum Likelihood Estimate for theta (joint) is:", theta_max_4))
print(paste("The maximum Probability for joint distribution is:", max_prob_4))
```
\

## Scenario 4
After analyzing the packets, where some packets were identified as "dirty," we estimated the total number of dirty packets (\(\theta\)) in a set of 496 packets. Two different approaches can be used for this estimation:

1. **Fixed Sample Method**: Drawing a fixed number of packets at random and analyzing them.
2. **Sequential Sampling Method**: Drawing packets one by one until the first dirty packet is found.

Using statistical inference techniques, we compared these two estimation methods by evaluating their **bias** and **root mean squared error (RMSE)** through a **Monte Carlo simulation**.

\


```{r, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# Load required libraries
library(ggplot2)
library(reshape2)
library(extraDistr)

# Set parameters
set.seed(0212.2025)
N_1 <- 496                
true_thetas <- c(5, 20, 50, 100, 150, 250, 331, 450)  # Different true values of theta to test
sample_sizes <- c(4, 5, 6, 7, 8)  # Sample sizes to test for MLE1
R <- 1000 

# Function to compute MLE for hypergeometric distribution with different sample sizes
compute_MLE1 <- function(true_theta, n) {
  X <- rhyper(R, true_theta, N_1 - true_theta, n)  # Generate R samples
  MLE_X <- numeric(R)

  for (i in 1:R) {
    possible_theta <- 0:N_1
    probability <- dhyper(X[i], possible_theta, N_1 - possible_theta, n)
    MLE_X[i] <- possible_theta[which.max(probability)]
  }

  avg_MLE <- mean(MLE_X)  # Compute the average MLE estimate

  return(list(MLE = MLE_X, mean_MLE = avg_MLE))
}

# Compute MLE1 for different values of true_theta and sample sizes
MLE1_samples <- list()
MLE1_means <- list()

for (n in sample_sizes) {
  MLE1_samples[[as.character(n)]] <- lapply(true_thetas, compute_MLE1, n = n)
  MLE1_means[[as.character(n)]] <- sapply(MLE1_samples[[as.character(n)]], function(x) x$mean_MLE)
}

# Function to compute MLE for negative hypergeometric distribution (MLE3)
compute_MLE3 <- function(true_theta) {
  Y <- rnhyper(R, N_1 - true_theta, true_theta, 1)  # Generate R samples
  MLE_Y <- numeric(R)

  for (i in 1:R) {
    possible_theta <- 1:(N_1 - 1)
    probability <- dnhyper(Y[i], N_1 - possible_theta, possible_theta, 1)
    MLE_Y[i] <- possible_theta[which.max(probability)]
  }

  avg_MLE3 <- mean(MLE_Y)  # Compute the average MLE3 estimate

  return(list(MLE = MLE_Y, mean_MLE = avg_MLE3))
}

# Compute MLE3 for different values of true_theta
MLE3_samples <- lapply(true_thetas, compute_MLE3)
MLE3_means <- sapply(MLE3_samples, function(x) x$mean_MLE)

# Function to compute Bias and RMSE
compute_bias_rmse <- function(MLE_samples, true_theta) {
  bias <- mean(MLE_samples) - true_theta
  rmse <- sqrt(var(MLE_samples) + (bias ** 2))
  return(c(bias, rmse))
}

# Compute Bias and RMSE for MLE1
MLE1_results <- list()
for (n in sample_sizes) {
  MLE1_results[[as.character(n)]] <- t(
    mapply(compute_bias_rmse, lapply(MLE1_samples[[as.character(n)]], function(x) x$MLE), true_thetas)
  )
}

# Compute Bias and RMSE for MLE3
MLE3_results <- t(mapply(compute_bias_rmse, lapply(MLE3_samples, function(x) x$MLE), true_thetas))

# Assign column names
colnames(MLE3_results) <- c("Bias_MLE3", "RMSE_MLE3")

# Convert MLE1 results into a dataframe
results_list <- list()
for (n in sample_sizes) {
  results_list[[as.character(n)]] <- data.frame(
    True_Theta = true_thetas,
    Bias_MLE1 = MLE1_results[[as.character(n)]][, 1],
    RMSE_MLE1 = MLE1_results[[as.character(n)]][, 2],
    Mean_MLE1 = MLE1_means[[as.character(n)]]  # Include average MLE estimates
  )
}

# Convert MLE3 results into a dataframe
MLE3_results_df <- data.frame(
  True_Theta = true_thetas,
  Bias_MLE3 = MLE3_results[, 1],
  RMSE_MLE3 = MLE3_results[, 2],
  Mean_MLE3 = MLE3_means  # Include average MLE estimates
)

# Convert MLE1 results into long format for plotting
results_long <- list()
for (n in sample_sizes) {
  results_long[[as.character(n)]] <- melt(
    results_list[[as.character(n)]], id.vars = "True_Theta"
  )
}

# Combine all sample sizes into a single data frame for Mean MLE, Bias, and RMSE (MLE1)
all_results_long <- do.call(rbind, Map(function(df, n) {
  df$Sample_Size <- as.character(n)
  return(df)
}, results_long, sample_sizes))

# Convert MLE3 results into long format for plotting
results_long_mle3 <- melt(MLE3_results_df, id.vars = "True_Theta")
results_long_mle3$Sample_Size <- "Sequential-n (MLE3)"

# Merge MLE1 and MLE3 results
all_results_combined <- rbind(all_results_long, results_long_mle3)

# Separate Bias, RMSE, and Mean MLE for combined plotting
bias_data <- subset(all_results_combined, variable %in% c("Bias_MLE1", "Bias_MLE3"))
rmse_data <- subset(all_results_combined, variable %in% c("RMSE_MLE1", "RMSE_MLE3"))
mean_data <- subset(all_results_combined, variable %in% c("Mean_MLE1", "Mean_MLE3"))

# Rename variable names for better readability
bias_data$variable <- gsub("Bias_MLE1", "MLE1 (Fixed-n)", bias_data$variable)
bias_data$variable <- gsub("Bias_MLE3", "MLE3 (Sequential-n)", bias_data$variable)
rmse_data$variable <- gsub("RMSE_MLE1", "MLE1 (Fixed-n)", rmse_data$variable)
rmse_data$variable <- gsub("RMSE_MLE3", "MLE3 (Sequential-n)", rmse_data$variable)
mean_data$variable <- gsub("Mean_MLE1", "MLE1 (Fixed-n)", mean_data$variable)
mean_data$variable <- gsub("Mean_MLE3", "MLE3 (Sequential-n)", mean_data$variable)

# Plot Bias
p_bias <- ggplot(bias_data, aes(x = True_Theta, y = value, color = Sample_Size, linetype = variable)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  labs(title = "Comparison of Bias for MLE1 and MLE3", x = "True Theta", y = "Bias")

print(p_bias)

# Plot RMSE
p_rmse <- ggplot(rmse_data, aes(x = True_Theta, y = value, color = Sample_Size, linetype = variable)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  labs(title = "Comparison of RMSE for MLE1 and MLE3", x = "True Theta", y = "RMSE")

print(p_rmse)

# Plot Mean MLE Estimates
p_mean <- ggplot(mean_data, aes(x = True_Theta, y = value, color = Sample_Size, linetype = variable)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  labs(title = "Comparison of Mean MLE for MLE1 and MLE3", x = "True Theta", y = "Mean MLE")

print(p_mean)
```

## **Comparison of MLE1 and MLE3**  

In this study, we compared two different Maximum Likelihood Estimators (MLEs) for \( \theta \) at different values of 20,60,150,250,331,and 450:  
1. **MLE1 (Fixed-\( n \) Estimate)**, where the sample size \( n \) is pre-determined.  
2. **MLE3 (Sequential-\( n \) Estimate)**, where sampling continues until at least one dirty packet is found.  

### **Comparison Metrics**  
- **Bias:** Measures the accuracy of the estimator. A lower bias means the estimator is closer to the true \( \theta \).  
- **RMSE (Root Mean Square Error):** Accounts for both accuracy and variability. A lower RMSE means the estimator is more stable and reliable.  

## **Conclusion from Our Analysis**  

### **Bias**  
- **MLE1 remains unbiased** across all values of \( \theta \), meaning it provides an accurate estimate of the true number of dirty packets.  
- **MLE3 is accurate when \( \theta \) is small**, but **starts to overestimate significantly** when \( \theta \) is large.  

### **RMSE**  
- **MLE1 has a lower and more stable RMSE**, meaning it consistently produces precise estimates.  
- **MLE3 has a low RMSE when \( \theta \) is small**, but **its RMSE increases when \( \theta \) is large**, indicating higher variability and inconsistency.  

## **Key Findings**  
- **MLE3 performs well when \( \theta \) is low**, as it **efficiently detects contamination without excessive sampling**. This makes it useful in **resource-constrained settings** where testing capacity is limited.  
- **MLE3 becomes unreliable when \( \theta \) is high**, as it **overestimates the number of contaminated packets**, leading to potential **false conclusions in forensic settings**.  
- **MLE1 remains stable across all values of \( \theta \)**, providing **consistent and dependable estimates**, making it a **preferred estimator when precision is critical**.  

In a forensic or legal setting, where **accurate estimation of contamination levels is crucial**, a judge would likely prefer **MLE1** due to its **stability and lack of overestimation bias**. While **MLE3 can be useful for early detection** when contamination is rare, its tendency to **overestimate high contamination cases makes it unreliable for critical decision-making**.  

Thus, **MLE1 is the recommended estimator for legal investigations** to ensure accurate and fair conclusions.  


## **Group Members' Contributions**  

- **Akin** - Answered part E (S4). Prepared results section of the report.  
- **David** - Answered question C and D. Prepared methods section of the report.  
- **Joshua** - Answered part E (S3). Prepared results section of the report.  
- **Prince** - Answered part E (S2). Prepared conclusion section of the report.  
- **Ravin** - Answered A and B of the problem. Prepared background section of the report.  

## Reference

Shuster, J. J. (1991). The statistician in a reverse cocaine sting. The American Statistician, 45(2),
123–124